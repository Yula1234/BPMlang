include "std"
include "tbpm/arena"
include "string"
include "string_view"

struct lexer {
	src:StringView,
	index:int,
	col:int
}

proc new_lexer src:ptr -> lexer {
	let src_sv = sv_from(src);
	return lexer(src_sv, 0, 1);
}

proc lexer_delete lxr:lexer -> void {
	delete lxr.src;
	delete lxr;
}

proc lexer_consume lxr:lexer -> int {
	let chr = sv_at(lxr.src, lxr.index);
	lxr.index = lxr.index + 1;
	lxr.col = lxr.col + 1;
	return chr;
}

const NULL_PEEK 4294967295;

proc lexer_peek lxr:lexer -> int {
	if(lxr.index > lxr.src.count) {
		return NULL_PEEK;
	}
	return sv_at(lxr.src, lxr.index);
}

proc lexer_peek_at lxr:lexer offs:int -> int {
	if((lxr.index + offs) > lxr.src.count) {
		return NULL_PEEK;
	}
	return sv_at(lxr.src, lxr.index + offs);
}

struct TmpStr {
	used:int,
	capacity:int,
	data:ptr
}

proc new_tmp_str arena:ArenaAllocator size:int -> TmpStr {
	return TmpStr(0, size, arena_take(arena, size));
}

proc tmp_str_realloc str:TmpStr arena:ArenaAllocator size:int -> void {
	str.used = 0;
	str.capacity = size;
	str.data = arena_take(arena, size);
}

proc tmp_str_push str:TmpStr chr:int -> void {
	store8(str.data + str.used, chr);
	str.used = str.used + 1;
	store8(str.data + str.used, 0);
}

proc isspace chr:int -> int {
	if(chr == ' ') {
		return true;
	}
	return false;
}

proc isdigit chr:int -> int {
	if(chr > ('0' - 1)) {
		if(chr < ('9' + 1)) {
			return true;
		}
		return false;
	}
	else {
		return false;
	}
}

proc is_identif chr:int -> int {
	if(chr == '_') {
		return true;
	}
	if(chr < 'A') {
		return false;
	}
	if(chr > 'z') {
		return false;
	}
	if(chr > 'Z') {
		if(chr < 'a') {
			return false;
		}
	}
	return true;
}

const TokenTypeIdent      iota;
const TokenTypeOpenParen  iota;
const TokenTypeCloseParen iota;
const TokenTypeExit       iota;
const TokenTypeIntLiteral iota;
const TokenTypeSemi       iota;
const TokenTypePlus       iota;
const TokenTypeMinus      iota;
const TokenTypeStar       iota;
const TokenTypeSlash      iota;
const TokenTypePrint      iota;
const TokenTypeLet        iota;
const TokenTypeEq         iota;
const TokenTypesCount     reset;

struct Token {
	type:int,
	line:int,
	col:int,
	value:ptr,
	file_loc:ptr,
}

proc push_token tokens:ptr index:ptr tok:Token -> void {
	store32(tokens + (rd32(index) * 4), cast(ptr, tok));
	store32(index, rd32(index) + 1);
}

proc tokentype2str type:int -> ptr {
	if(type == TokenTypeIdent) {
		return "`ident`";
	}
	elif(type == TokenTypeOpenParen) {
		return "`(`";
	}
	elif(type == TokenTypeCloseParen) {
		return "`)`";
	}
	elif(type == TokenTypeExit) {
		return "`exit`";
	}
	elif(type == TokenTypeOpenParen) {
		return "`(`";
	}
	elif(type == TokenTypeIntLiteral) {
		return "`int literal`";
	}
	elif(type == TokenTypeOpenParen) {
		return "`(`";
	}
	elif(type == TokenTypeSemi) {
		return "`;`";
	}
	elif(type == TokenTypePlus) {
		return "`+`";
	}
	elif(type == TokenTypeMinus) {
		return "`-`";
	}
	elif(type == TokenTypeStar) {
		return "`*`";
	}
	elif(type == TokenTypeSlash) {
		return "`/`";
	}
	elif(type == TokenTypePrint) {
		return "`print`";
	}
	elif(type == TokenTypeLet) {
		return "`let`";
	}
	elif(type == TokenTypeEq) {
		return "`=`";
	}
	format_print("tokentype2str error: unkown TokenType `%d`\n", type);
	exit(0);
}

proc print_token tok:Token -> void {
	if(strlen(tok.value) != 0) {
		format_print("Token(%s, %s:%d:%d, %s)\n", tokentype2str(tok.type), tok.file_loc, tok.line, tok.col, tok.value);
	} else {
		format_print("Token(%s, %s:%d:%d)\n", tokentype2str(tok.type), tok.file_loc, tok.line, tok.col);
	}
}

proc print_tokens tokens:ptr -> void {
	let i = 0;
	while(rd32(tokens + (i * 4)) != 0) {
		let tok = cast(Token, rd32(tokens + (i * 4)));
		print_token(tok);
		i = i + 1;
	}
}

proc free_tokens tokens:ptr -> void {
	let i = 0;
	while(rd32(tokens + (i * 4)) != 0) {
		let tok = cast(Token, rd32(tokens + (i * 4)));
		delete tok;
		i = i + 1;
	}
	memfree(tokens);
}

proc lexer_lex
	lxr:lexer
	str_arena:ArenaAllocator
	tokens:ptr
	FILE_PATH:ptr
-> void 
{
	let tokens_count = 0;
	let line_count = 1;
	let tmp_tok: Token;
	let tmp_str = new_tmp_str(str_arena, 56);
	while(lexer_peek(lxr) != NULL_PEEK) {
		if(is_identif(lexer_peek(lxr))) {
			while(is_identif(lexer_peek(lxr))) {
				tmp_str_push(tmp_str, lexer_consume(lxr));
			}
			if(streq(tmp_str.data, "exit")) {
				tmp_tok = Token(TokenTypeExit, line_count, lxr.col - strlen(tmp_str.data), "", FILE_PATH);
				push_token(tokens, &tokens_count, tmp_tok);
			}
			elif(streq(tmp_str.data, "print")) {
				tmp_tok = Token(TokenTypePrint, line_count, lxr.col - strlen(tmp_str.data), "", FILE_PATH);
				push_token(tokens, &tokens_count, tmp_tok);
			}
			elif(streq(tmp_str.data, "let")) {
				tmp_tok = Token(TokenTypeLet, line_count, lxr.col - strlen(tmp_str.data), "", FILE_PATH);
				push_token(tokens, &tokens_count, tmp_tok);
			}
			else {
				tmp_tok = Token(TokenTypeIdent, line_count, lxr.col - strlen(tmp_str.data), tmp_str.data, FILE_PATH);
				push_token(tokens, &tokens_count, tmp_tok);
			}
			tmp_str_realloc(tmp_str, str_arena, 56);
		}
		elif(lexer_peek(lxr) == '(') {
			tmp_tok = Token(TokenTypeOpenParen, line_count, lxr.col - 1, "", FILE_PATH);
			push_token(tokens, &tokens_count, tmp_tok);
			lexer_consume(lxr);
		}
		elif(lexer_peek(lxr) == ')') {
			tmp_tok = Token(TokenTypeCloseParen, line_count, lxr.col - 1, "", FILE_PATH);
			push_token(tokens, &tokens_count, tmp_tok);
			lexer_consume(lxr);
		}
		elif(lexer_peek(lxr) == ';') {
			tmp_tok = Token(TokenTypeSemi, line_count, lxr.col - 1, "", FILE_PATH);
			push_token(tokens, &tokens_count, tmp_tok);
			lexer_consume(lxr);
		}
		elif(lexer_peek(lxr) == '+') {
			tmp_tok = Token(TokenTypePlus, line_count, lxr.col - 1, "", FILE_PATH);
			push_token(tokens, &tokens_count, tmp_tok);
			lexer_consume(lxr);
		}
		elif(lexer_peek(lxr) == '-') {
			tmp_tok = Token(TokenTypeMinus, line_count, lxr.col - 1, "", FILE_PATH);
			push_token(tokens, &tokens_count, tmp_tok);
			lexer_consume(lxr);
		}
		elif(lexer_peek(lxr) == '*') {
			tmp_tok = Token(TokenTypeStar, line_count, lxr.col - 1, "", FILE_PATH);
			push_token(tokens, &tokens_count, tmp_tok);
			lexer_consume(lxr);
		}
		elif(lexer_peek(lxr) == '=') {
			tmp_tok = Token(TokenTypeEq, line_count, lxr.col - 1, "", FILE_PATH);
			push_token(tokens, &tokens_count, tmp_tok);
			lexer_consume(lxr);
		}
		elif(lexer_peek(lxr) == '/') {
			tmp_tok = Token(TokenTypeSlash, line_count, lxr.col - 1, "", FILE_PATH);
			push_token(tokens, &tokens_count, tmp_tok);
			lexer_consume(lxr);
		}
		elif(isdigit(lexer_peek(lxr))) {
			while(isdigit(lexer_peek(lxr))) {
				tmp_str_push(tmp_str, lexer_consume(lxr));
			}
			tmp_tok = Token(TokenTypeIntLiteral, line_count, lxr.col - strlen(tmp_str.data), tmp_str.data, FILE_PATH);
			push_token(tokens, &tokens_count, tmp_tok);
			tmp_str_realloc(tmp_str, str_arena, 56);
		}
		elif(lexer_peek(lxr) == '\n') {
			line_count = line_count + 1;
			lexer_consume(lxr);
		}
		elif(isspace(lexer_peek(lxr))) {
			while(isspace(lexer_peek(lxr))) {
				lexer_consume(lxr);
			}
		}
		else {
			if(lexer_peek(lxr) != 0) {
				format_print("%s:%d:%d ERROR: invalid token\n", FILE_PATH, line_count, lxr.col);
				exit(1);
			}
			lexer_consume(lxr); // 0 byte
		}
	}
	store32(tokens + (tokens_count * 4), 0);
}